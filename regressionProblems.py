# -*- coding: utf-8 -*-
"""HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LoAxvm40LUDGNL5BrhbhnMKfyd7Ng0PY
"""

# -*- coding: utf-8 -*-
"""singleOutputRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HyjqdZNhbJSxnaDtvICyZH_yrjXqlxEa
"""

# necessary python packages
import numpy as np                 #helps with numerical handling
import matplotlib.pyplot as plt    #helps with plots
import scipy.optimize as opt       #curve fitting tools
from numpy.polynomial.polynomial import polyval   #Evaluate a polynomial at points x
from sklearn.metrics import mean_absolute_error, mean_squared_error,max_error #the regression related metric scores



# 1o EROTHMA

# generate some random values
numberOfGeneratedValues = 150
inputValues = np.random.uniform(-4,4, numberOfGeneratedValues)
# sort results
inputValues.sort()

# 2o EROTHMA

def myCustFunc(x,p1,p2):
  y = np.array(p1*(1/np.exp(x))+p2*np.sin(x)) # y size = x size
  return y

# 3o EROTHMA

L1=0.2
L2=4.5

# create some output values using the defined functions
myCustFuncValues = myCustFunc(inputValues,L1,L2)

# 4o EROTHMA

# start plotting the results
plt.plot(inputValues, 'r+', label='random uniform')
plt.plot(myCustFuncValues, 'b1', label='myCustFunc')
plt.xlabel ('index of inputValues')
plt.ylabel ('output')
plt.title ('Current values')
plt.legend(loc='best')
plt.show()
# second plot for compact demonstration
plt.plot(inputValues,myCustFuncValues, 'g3', label='results')
plt.xlabel ('input')
plt.ylabel ('output')
plt.title ('myCustFunc outputs')
plt.legend(loc='best')
plt.show()

# 5o EROTHMA

# noisy values
noisyValues = myCustFuncValues + np.random.poisson(11, 150)

# 6o EROTHMA


# now use a curve fitting tools of scipy
# carefull: works if you know the model!

# bestValsCustModel[0] = λ1, [1] = λ2

#original data
bestValsLinModel, CoVarLinModel =\
 opt.curve_fit(myCustFunc,inputValues,myCustFuncValues)
print('muCustFunc model estimated parameters (on original data) are:',\
      '{:.2f}'.format(bestValsLinModel[0]),\
      '{:.2f}'.format(bestValsLinModel[1]))

# noisy data
bestValsCustModel, CoVarLinModel = opt.curve_fit(myCustFunc,inputValues,noisyValues)
print('myCustFunc model estimated parameters (on noisy data) are:',\
      '{:.2f}'.format(bestValsCustModel[0]),\
      '{:.2f}'.format(bestValsCustModel[1]))

# 7o EROTHMA

# Creating a 4th degree polynomial
# pol = a*x^4 + b*x^3 + c*x^2 + d*x + e
def poly4thDegree(x, a, b, c, d, e):
  pol=np.array(polyval(x,[e,d,c,b,a]))
  return pol

#8o EROTHMA

# Calculate parameters of the polynomial
bestValsPolModel, CoVarPolModel =  opt.curve_fit(poly4thDegree,inputValues,noisyValues)
# print parameters
print('poly4thDegree model estimated parameters (on noisy data) are:',\
      'a = {:.2f}'.format(bestValsPolModel[0]),\
      'b = {:.2f}'.format(bestValsPolModel[1]),\
      'c = {:.2f}'.format(bestValsPolModel[2]),\
      'd = {:.2f}'.format(bestValsPolModel[3]),\
      'e = {:.2f}'.format(bestValsPolModel[4]))

# 9a EROTHMA

myCustPredictedOutcomes = myCustFunc(inputValues,bestValsCustModel[0],bestValsCustModel[1])
plt.plot( inputValues,myCustPredictedOutcomes,'b*', label='Predicted Values')
plt.plot(inputValues,noisyValues, 'g.', label='Noisy Values')
plt.xlabel ('input')
plt.ylabel ('output')
plt.title ('MyCustFunc Prediction')
plt.legend(loc='best')
plt.show()
# plt.plot(outputValues1, 'b.', label='Actual')

# 9b EROTHMA

#calculate the outputs using the curve fiting estimated parameters
poly4thDegreePredictedOutcomes = poly4thDegree(inputValues,bestValsPolModel[0],bestValsPolModel[1],bestValsPolModel[2],bestValsPolModel[3],bestValsPolModel[4])
plt.plot(inputValues,poly4thDegreePredictedOutcomes, 'b*', label='Predicted Values')
plt.plot(inputValues,noisyValues, 'g.', label='Noisy Values')
plt.xlabel ('input')
plt.ylabel ('output')
plt.title ('Poly4thDegree Prediction')
plt.legend(loc='best')
plt.show()

# 10o EROTHMA

mae_original = mean_absolute_error(myCustFuncValues,noisyValues)
rmse_original = np.sqrt((mean_squared_error(myCustFuncValues,noisyValues)))
print('the scores at original_values-noisy_values are:',\
      'MAE: {:.2f}'.format(mae_original),\
      'RMSE: {:.2f}'.format(rmse_original))

mae_myCustFunc = mean_absolute_error(noisyValues,myCustPredictedOutcomes)
rmse_myCustFunc = np.sqrt((mean_squared_error(noisyValues,myCustPredictedOutcomes)))
print('the scores for the myCustFunc on noisy data are:',\
      'MAE: {:.2f}'.format(mae_myCustFunc),\
      'RMSE: {:.2f}'.format(rmse_myCustFunc))

mae_poly4thDegre = mean_absolute_error(noisyValues,poly4thDegreePredictedOutcomes)
rmse_poly4thDegre = np.sqrt((mean_squared_error(noisyValues,poly4thDegreePredictedOutcomes)))
print('the scores for the poly4thDegree on noisy data are:',\
      'MAE: {:.2f}'.format(mae_poly4thDegre),\
      'RMSE: {:.2f}'.format(rmse_poly4thDegre))

#-----MEROS 2 -----

# step 3: choose a model to fit (train) on data
# use a kNN regressor
from sklearn.neighbors import KNeighborsRegressor
# use an SVR
from sklearn.svm import SVR
# use decision tree
from sklearn.tree import DecisionTreeRegressor

#-------------------- 1 --------------------

#reshape inputValues to be a 2D array
inputValues = inputValues.reshape(-1,1)

#-------------------- 2 --------------------

# step 2: create train, validation, and test data (indices)
# we want to avoid duplicating (copying) existing set
trainDatPerc = 0.7
valDatPerc = trainDatPerc/10
testDatPec = 1 - trainDatPerc - valDatPerc

# create shufled indexes for the data 
indices = np.random.permutation(len(inputValues))
trainInd = indices[0:int(trainDatPerc*len(indices))]
valInd = indices[int(trainDatPerc*len(indices)+1):\
                 int((trainDatPerc+valDatPerc)*len(indices))]
testInd = indices[int((trainDatPerc+valDatPerc)*len(indices))+1:]

# --------------------3--------------------

# Initialize a model with 5 neighbors
knnReg = KNeighborsRegressor(n_neighbors=5)
# Fit the model
knnReg.fit(inputValues[trainInd],noisyValues[trainInd])
# Initialize a model with rbf kernel
svmReg =SVR(kernel='rbf')
# Fit the model
svmReg.fit(inputValues[trainInd],noisyValues[trainInd])
# Initialize a decision tree regressor
treeReg = DecisionTreeRegressor()
# Fit the model
treeReg = treeReg.fit(inputValues[trainInd],noisyValues[trainInd])

# plot training set original and predicted output values

# first calculate predicted outputs (for train set only)
predictedOutputsknn = knnReg.predict(inputValues[trainInd])
predictedOutputssvm = svmReg.predict(inputValues[trainInd])
predictedOutputstree = treeReg.predict(inputValues[trainInd])

# --------------------4--------------------

# now the ploting
plt.figure(figsize=(12, 4)) # define size to better illustrate results
plt.plot(noisyValues[trainInd],'ro', label = 'actual',markersize=7)
plt.plot(predictedOutputsknn,'kx', label = 'knn',markersize=5)
plt.plot(predictedOutputssvm,'g+', label = 'svr')
plt.plot(predictedOutputstree,'b*', label = 'tree', markersize=5)
plt.title ('Predicted Values for train set')
plt.xlabel ('indexInputValues index')
plt.ylabel ('predictedOutput index')
plt.legend(loc='best')
plt.show()

# then calculate predicted outputs (for test set)
predictedOutputsknn = knnReg.predict(inputValues[testInd])
predictedOutputssvm = svmReg.predict(inputValues[testInd])
predictedOutputstree = treeReg.predict(inputValues[testInd])

# now the ploting
plt.figure(figsize=(12, 4)) # define size to better illustrate results
plt.plot(noisyValues[testInd],'ro', label = 'actual',markersize=7)  # make bigger the marker size for better illustration of the results
plt.plot(predictedOutputsknn,'kx', label = 'knn')
plt.plot(predictedOutputssvm,'g+', label = 'svr')
plt.plot(predictedOutputstree,'b*', label = 'tree')
plt.legend(loc='best')
plt.xlabel ('inputValues index')
plt.ylabel ('predictedOutput index')
plt.title ('Predicted Values for test set')
plt.show()

# calculate the scores
mae_knn = mean_absolute_error(noisyValues[testInd], predictedOutputsknn)
rmse_knn = np.sqrt(mean_squared_error(noisyValues[testInd],\
                                      predictedOutputsknn))
max_knn = max_error(noisyValues[testInd], predictedOutputsknn)


mae_svm = mean_absolute_error(noisyValues[testInd], predictedOutputssvm)
rmse_svm = np.sqrt(mean_squared_error(noisyValues[testInd],\
                                      predictedOutputssvm))
max_svm = max_error(noisyValues[testInd], predictedOutputssvm)

mae_tree = mean_absolute_error(noisyValues[testInd], predictedOutputstree)
rmse_tree = np.sqrt(mean_squared_error(noisyValues[testInd],predictedOutputstree))
max_tree = max_error(noisyValues[testInd], predictedOutputstree)


# print them on screen
print('the knn regresor (test) scores are:',\
      'MAE: {:.2f}'.format(mae_knn),\
      'RMSE: {:.2f}'.format(rmse_knn),\
      'max error: {:.2f}'.format(max_knn))

print('the svm regresor (test) scores are:',\
      'MAE: {:.2f}'.format(mae_svm),\
      'RMSE: {:.2f}'.format(rmse_svm),\
      'max error: {:.2f}'.format(max_svm))

print('the decision tree regresor (test) scores are:',\
      'MAE: {:.2f}'.format(mae_tree),\
      'RMSE: {:.2f}'.format(rmse_tree),\
      'max error: {:.2f}'.format(max_tree))

# --------------------5--------------------

from sklearn.preprocessing import MinMaxScaler
scaler_inputs = MinMaxScaler()
# normalize inputValues
inputValues = scaler_inputs.fit_transform(inputValues)

# create shufled indexes for the data 
indices = np.random.permutation(len(inputValues))
trainInd = indices[0:int(trainDatPerc*len(indices))]
valInd = indices[int(trainDatPerc*len(indices)+1):\
                 int((trainDatPerc+valDatPerc)*len(indices))]
testInd = indices[int((trainDatPerc+valDatPerc)*len(indices))+1:]

# choose a model to fit (train) on data
# use a kNN regressor

# Initialize a model with 5 neighbors
knnReg = KNeighborsRegressor(n_neighbors=5)
# Fit the model
knnReg.fit(inputValues[trainInd],noisyValues[trainInd])
# Initialize a model with rbf kernel
svmReg =SVR(kernel='rbf')
# Fit the model
svmReg.fit(inputValues[trainInd],noisyValues[trainInd])
# Initialize a decision tree regressor
treeReg = DecisionTreeRegressor()
# Fit the model
treeReg = treeReg.fit(inputValues[trainInd],noisyValues[trainInd])

# first calculate predicted outputs (for train set only)
predictedOutputsknn = knnReg.predict(inputValues[trainInd])
predictedOutputssvm = svmReg.predict(inputValues[trainInd])
predictedOutputstree = treeReg.predict(inputValues[trainInd])

# now the ploting
plt.figure(figsize=(12, 4)) # define size to better illustrate results
plt.plot(noisyValues[trainInd],'ro', label = 'actual',markersize=7)
plt.plot(predictedOutputsknn,'kx', label = 'knn', markersize=5)
plt.plot(predictedOutputssvm,'g+', label = 'svr')
plt.plot(predictedOutputstree,'b*', label = 'tree', markersize=5)
plt.legend(loc='best')
plt.title ('Predicted Values for train set')
plt.xlabel ('inputValues index')
plt.ylabel ('predictedOutput index')
plt.show()


# calculate predicted outputs (for test set)
predictedOutputsknn = knnReg.predict(inputValues[testInd])
predictedOutputssvm = svmReg.predict(inputValues[testInd])
predictedOutputstree = treeReg.predict(inputValues[testInd])

#now the ploting
plt.figure(figsize=(12, 4)) # define size to better illustrate results
plt.plot(noisyValues[testInd],'ro', label = 'actual',markersize=7)
plt.plot(predictedOutputsknn,'kx', label = 'knn')
plt.plot(predictedOutputssvm,'g+', label = 'svr')
plt.plot(predictedOutputstree,'b*', label = 'tree')
plt.legend(loc='best')
plt.title ('Predicted Values for train set')
plt.xlabel ('inputValues index')
plt.ylabel ('predictedOutput index')
plt.show()

# calculate the scores
mae_knn = mean_absolute_error(noisyValues[testInd], predictedOutputsknn)
rmse_knn = np.sqrt(mean_squared_error(noisyValues[testInd],\
                                      predictedOutputsknn))
max_knn = max_error(noisyValues[testInd], predictedOutputsknn)


mae_svm = mean_absolute_error(noisyValues[testInd], predictedOutputssvm)
rmse_svm = np.sqrt(mean_squared_error(noisyValues[testInd],\
                                      predictedOutputssvm))
max_svm = max_error(noisyValues[testInd], predictedOutputssvm)

mae_tree = mean_absolute_error(noisyValues[testInd], predictedOutputstree)
rmse_tree = np.sqrt(mean_squared_error(noisyValues[testInd],predictedOutputstree))
max_tree = max_error(noisyValues[testInd], predictedOutputstree)


#print them on screen
print('the knn regresor (test) scores are:',\
      'MAE: {:.2f}'.format(mae_knn),\
      'RMSE: {:.2f}'.format(rmse_knn),\
      'max error: {:.2f}'.format(max_knn))

print('the svm regresor (test) scores are:',\
      'MAE: {:.2f}'.format(mae_svm),\
      'RMSE: {:.2f}'.format(rmse_svm),\
      'max error: {:.2f}'.format(max_svm))

print('the decision tree regresor (test) scores are:',\
      'MAE: {:.2f}'.format(mae_tree),\
      'RMSE: {:.2f}'.format(rmse_tree),\
      'max error: {:.2f}'.format(max_tree))